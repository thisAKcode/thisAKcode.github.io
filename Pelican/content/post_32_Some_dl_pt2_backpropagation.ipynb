{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25434fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "- title: Some DL Pt2. Back Propagation.\n",
    "- author: Alex\n",
    "- date: 2024-10-21\n",
    "- category: python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b4838",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "When in ml you get the error you need to update parameters (weights and biases) in order to minimize it. This is done by using the gradient of the error with respect to the parameters. This is done by using the chain rule of calculus:\n",
    "function f(g(x)) has partial derivatives with respect to x f'(g(x)) * g'(x). \n",
    "\n",
    "The error is a function of the output of the neural network and the output of the neural network is a function of the weights and biases. So the derivative of the error with respect to the weights and biases is the derivative of the error with respect to the output of the neural network times the derivative of the output of the neural network with respect to the weights and biases. Psst... chain rule. Alltogehter this is called backpropagation, pesudo code looks like this:\n",
    "```python\n",
    "output = neural_network(input, weights, biases)\n",
    "error = loss(output, target)\n",
    "gradient = error_gradient(output, target)\n",
    "weights, biases = backpropagate(gradient, weights, biases)\n",
    "```\n",
    "\n",
    "We calculate our error by taking the diff (actual - expected) and getting back along the net at small learning rates adjusting each of the matrices of weights and biases.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
