{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25434fd",
   "metadata": {},
   "source": [
    "- title: Some DL Pt2. Back Propagation.\n",
    "- author: Alex\n",
    "- date: 2024-10-21\n",
    "- category: python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b4838",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "To unrderstand backpropagation we need to understand the forward propagation: the process of moving the input data through the net to get the output.\n",
    "On the contrary, backpropagation is the process of moving the error back through the net to adjust the weights and biases.\n",
    "Backpropagation involves calculating the gradient of the error with respect to the weights and biases of the neural network.\n",
    " \n",
    "### Chain Rule in Calculus\n",
    "When in ml you get the error you need to update parameters (weights and biases) in order to minimize it. This is done by using the gradient of the error with respect to the parameters. This is done by using the chain rule of calculus:\n",
    "function f(g(x)) has partial derivatives with respect to x f'(g(x)) * g'(x). \n",
    "\n",
    "\n",
    "Let's apply the chain rule of calculus to the given context.\n",
    "The error is a function of the output of the neural network and the output of the neural network is a function of the weights and biases.\n",
    "1. Identify the outer function  f  and the inner function $g$.\n",
    "2. Replace f with \"derivative of the error with respect to the weights and biases\".\n",
    "3. Replace g with \"derivative of the error with respect to the output of the neural network\".\n",
    "4. Apply the chain rule: $$ f(g(x))' = f'(g(x)) \\cdot g'(x) $$.\n",
    "\n",
    "### Chain Rule Applied to Backpropagation\n",
    "Given:\n",
    "- $f$ = \"derivative of the error with respect to the weights and biases\"\n",
    "- $g$ = \"derivative of the error with respect to the output of the neural network\"\n",
    "\n",
    "The chain rule states:\n",
    "$[ \\frac{d}{dx} f(g(x)) = f'(g(x)) \\cdot g'(x) ]$\n",
    "\n",
    "Which is in this context is:\n",
    "$[ \\frac{dE}{dW} = \\frac{dE}{dO} \\cdot \\frac{dO}{dW} ]$\n",
    "\n",
    "Where:\n",
    "- $E$ is the error.\n",
    "- $O$ is the output of the neural network.\n",
    "- $W$ represents the weights and biases.\n",
    " \n",
    " So the derivative of the error with respect to the weights and biases is the derivative of the error with respect to the output of the neural network times the derivative of the output of the neural network with respect to the weights and biases. Psst... chain rule. Alltogehter this is called backpropagation, \n",
    "### In short\n",
    "We calculate our error by taking the diff (actual - expected) and getting back along the net at small learning rates adjusting each of the matrices of weights and biases in a Net.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d20390d",
   "metadata": {},
   "source": [
    "in this article I tried to get gradient descent <https://www.prettylagom.me/regression.html> in python. The implementation of gradient descent in logistic regression is highly relevant here since it is algorithm used to minimize the loss function in various machine learning models, including deep learning models. \n",
    "\n",
    "The pesudo code is below. By looking at this you see multiplicatino of the derivative of the function by the derivative of the inside(== just follows Chain Rule). \n",
    "\n",
    "```python\n",
    "# Forward pass\n",
    "output = neural_network(input, weights, biases)\n",
    "# Compute the loss function\n",
    "error = loss(output, target)\n",
    "# Backward pass\n",
    "gradient = error_gradient(output, target)\n",
    "# Update the weights and biases\n",
    "weights, biases = backpropagate(gradient, weights, biases)\n",
    "```\n",
    "\n",
    "In real implementation it is iterated over multiple epochs to minimize the loss function and improve the model's performance. \n",
    "```python\t\n",
    "import numpy as np\n",
    "\n",
    "# Predict\n",
    "# Compute the linear combination of the weights and input features\n",
    "linear_combination = np.dot(weights.T, input_features) + bias\n",
    "\n",
    "# Apply the sigmoid activation function to get the predicted probabilities\n",
    "activation_output = sigmoid(linear_combination)\n",
    "\n",
    "# Gradient descent\n",
    "# Compute the difference between the predicted probabilities and the actual labels\n",
    "error = activation_output - true_labels\n",
    "\n",
    "# Compute the gradient of the loss with respect to the weights\n",
    "gradient_weights = (1/num_samples) * np.dot(input_features, error.T)\n",
    "\n",
    "# Compute the gradient of the loss with respect to the bias\n",
    "gradient_bias = (1/num_samples) * np.sum(error)\n",
    "\n",
    "# Update\n",
    "# Update the weights by subtracting the product of the learning rate and the gradient\n",
    "weights = weights - learning_rate * gradient_weights\n",
    "\n",
    "# Update the bias similarly\n",
    "bias = bias - learning_rate * gradient_bias\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d394099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442191d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
