{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- title: Data QA Using Python\n",
    "- author: Alex\n",
    "- date: 2025-10-26\n",
    "- category: python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data QA Using Python \n",
    "A data quality analysis (QA) problem over a table using pandas without drowning in code.Let’s discover a systematic QA analysis, step-by-step.\n",
    "\n",
    "You have a table ith these columns:\n",
    "\n",
    "| A | B | C | width | D | Region |\n",
    "\n",
    "Each row represents record. Region groups related records geografically.\n",
    "\n",
    "A, B, C, width, and D are attributes to test for completeness, consistency, and validity.\n",
    "\n",
    "### Domain Knowledge\n",
    "\n",
    "You need to have some domain knowledge to not make some blueprint analysis just based on the method itself. i.e. in this example the width cannot be too small, group by can be performed by region. It can be some other rules like if I have noticed:\n",
    "\n",
    "A → sometimes empty\n",
    "\n",
    "B → often empty\n",
    "\n",
    "C → may have false positives\n",
    "\n",
    "width → presumably numeric (complete?)\n",
    "\n",
    "Region → categorical, useful for grouping\n",
    "\n",
    "D (another attribute?) → complete in some Region groups\n",
    "\n",
    "\n",
    "###  Objective\n",
    "Goal is to measure how good or reliable this data is according to some rules - generally get a sense of completeness, validity, uniqueness, and consistency.\n",
    "\n",
    "- Detect missing data (A, B, width)\n",
    "\n",
    "- Flag possible false positives in C\n",
    "\n",
    "- Check completeness of D per Region\n",
    "\n",
    "- Summarize QA metrics per region\n",
    "\n",
    "All this is a form of data quality assessment — which is a component of profiling which probably is the topic for future post. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Metrics\n",
    "\n",
    "\n",
    "|       Check       |                  What it shows                  |           Example          |\n",
    "|:-----------------:|:-----------------------------------------------:|:--------------------------:|\n",
    "| Count / Missing % | How much data is missing                        | df.isna().sum()            |\n",
    "| Distinct count    | How varied the data is                          | df.nunique()               |\n",
    "| Range check       | Are numeric values within expected bounds       | e.g., width > 3.0            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design  Python (pandas) QA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices with null values: [114  62  33 107   7 100  40  86  76  71 134]\n",
      "    sepal length (cm)  sepal width (cm)    Region\n",
      "0                 5.1               3.5  Region_A\n",
      "1                 4.9               3.0  Region_A\n",
      "2                 4.7               3.2  Region_C\n",
      "3                 4.6               3.1  Region_C\n",
      "4                 5.0               3.6  Region_A\n",
      "5                 5.4               3.9  Region_A\n",
      "6                 4.6               3.4  Region_A\n",
      "7                 5.0               NaN  Region_B\n",
      "8                 4.4               2.9  Region_C\n",
      "9                 4.9               3.1  Region_A\n",
      "10                5.4               3.7  Region_A\n",
      "11                4.8               3.4  Region_B\n",
      "12                4.8               3.0  Region_A\n",
      "13                4.3               3.0  Region_C\n",
      "14                5.8               4.0  Region_B\n",
      "15                5.7               4.4  Region_B\n",
      "16                5.4               3.9  Region_B\n",
      "17                5.1               3.5  Region_A\n",
      "18                5.7               3.8  Region_A\n",
      "19                5.1               3.8  Region_A\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df_mock_qa_problem = iris_df.copy()\n",
    "# get only the first two features for simplicity \n",
    "iris_df = iris_df.iloc[:, :2]\n",
    "# add randomly Null values for sepal width\n",
    "np.random.seed(0) # for reproducibility\n",
    "\n",
    "null_indices = np.random.choice(iris_df.index, size=11, replace=False)\n",
    "print(\"Indices with null values:\", null_indices)\n",
    "iris_df.loc[null_indices, 'sepal width (cm)'] = np.nan\n",
    "# add a bogus categorical region column randomly\n",
    "bogus_regions = ['Region_A', 'Region_B', 'Region_C']\n",
    "iris_df['Region'] = np.random.choice(bogus_regions, size=len(iris_df))\n",
    "print(iris_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# --- 4 Consistency check: D should be consistent within Region ---\\nregion_D_consistency = (\\n  df.groupby(\"Region\")[\"D\"]\\n    .nunique()\\n    .reset_index(name=\"distinct_D_values\")\\n)\\nregion_D_consistency[\"consistent_D\"] = region_D_consistency[\"distinct_D_values\"] == 1\\n\\n# --- 5 Optional: QA scoring per Region ---\\nregion_qa = (\\n  df.groupby(\"Region\")\\n    .apply(lambda g: 1 - (g[[\"A\",\"B\",\"width\"]].isnull().sum().sum() / (len(g)*3)))\\n    .reset_index(name=\"qa_score\")\\n)\\nregion_qa[\"qa_label\"] = pd.cut(region_qa[\"qa_score\"],\\n                 bins=[0,0.7,0.9,1],\\n                 labels=[\"Poor\",\"Fair\",\"Good\"])\\n\\n# --- 6 Optional: False positive analysis for C ---\\nfalse_pos_candidates = df[df[\"C\"] == True]\\n\\n# --- Output results using click style ---\\nclick.secho(\"=== Overall Completeness ===\", fg=\"cyan\", bold=True)\\nclick.echo(completeness)\\nclick.secho(\"\\n=== Missing % by Region ===\", fg=\"cyan\", bold=True)\\nclick.echo(region_completeness)\\nclick.secho(\"\\n=== Validity Checks ===\", fg=\"cyan\", bold=True)\\nclick.echo(validity.T)\\nclick.secho(\"\\n=== Consistency of D per Region ===\", fg=\"cyan\", bold=True)\\nclick.echo(region_D_consistency)\\nclick.secho(\"\\n=== QA Score per Region ===\", fg=\"cyan\", bold=True)\\nclick.echo(region_qa)\\nclick.secho(\"\\n=== Sample suspected false positives in C ===\", fg=\"cyan\", bold=True)\\nclick.echo(false_pos_candidates.head(10))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import click\n",
    "df = iris_df\n",
    "# --- 1 Basic completeness checks ---\n",
    "completeness = df.isnull().mean().to_frame(\"missing_pct\") * 100\n",
    "completeness[\"non_missing_pct\"] = 100 - completeness[\"missing_pct\"]\n",
    "\n",
    "# --- 2 Groupwise completeness by Region ---\n",
    "region_completeness = (\n",
    "  df.groupby(\"Region\")\n",
    "    .apply(lambda g: g.isnull().mean() * 100)\n",
    "    .rename_axis(index=None)\n",
    ")\n",
    "\n",
    "# --- 3 Validity checks ---\n",
    "validity = pd.DataFrame()\n",
    "validity[\"width_invalid\"] = (df[\"sepal width (cm)\"] <= 0).sum()\n",
    "validity[\"width_null\"] = df[\"sepal width (cm)\"].isnull().sum()\n",
    "valid_C_values = df[\"sepal length (cm)\"].isin([True, False, 0, 1]).mean() * 100\n",
    "validity[\"C_valid_pct\"] = valid_C_values\n",
    "\"\"\"\n",
    "# --- 5 Optional: QA scoring per Region ---\n",
    "region_qa = (\n",
    "  df.groupby(\"Region\")\n",
    "    .apply(lambda g: 1 - (g[[\"A\",\"B\",\"width\"]].isnull().sum().sum() / (len(g)*3)))\n",
    "    .reset_index(name=\"qa_score\")\n",
    ")\n",
    "region_qa[\"qa_label\"] = pd.cut(region_qa[\"qa_score\"],\n",
    "                 bins=[0,0.7,0.9,1],\n",
    "                 labels=[\"Poor\",\"Fair\",\"Good\"])\n",
    "\n",
    "# --- 6 Optional: False positive analysis for C ---\n",
    "false_pos_candidates = df[df[\"C\"] == True]\n",
    "\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   missing_pct  non_missing_pct\n",
      "sepal length (cm)     0.000000       100.000000\n",
      "sepal width (cm)      7.333333        92.666667\n",
      "Region                0.000000       100.000000\n"
     ]
    }
   ],
   "source": [
    "print(completeness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus LLN-theory Application in QA\n",
    "I have no time to develop this further, but here is an interesting idea:\n",
    "\n",
    " You want to use pandas to check whether the distribution of a categorical variable A is consistent across regions (B).\n",
    "Great application to the idea behind the Law of Large Numbers: if data generation is consistent, the frequency of categories in A should be roughly stable across large-enough subsets (regions).\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Example: assume df has columns \"A\" (category) and \"B\" (region)\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "# 1. Overall distribution of A\n",
    "overall_dist = df['A'].value_counts(normalize=True)\n",
    "\n",
    "# 2. Regional distribution of A\n",
    "regional_dist = (\n",
    "    df.groupby('B')['A']\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# 3. Compare each region’s distribution to the overall one\n",
    "# Compute absolute deviation for each category\n",
    "deviation = (regional_dist - overall_dist).abs()\n",
    "\n",
    "# 4. Flag regions with large deviation\n",
    "threshold = 0.05  # e.g. more than 5 percentage points off\n",
    "flagged_regions = deviation.max(axis=1)[deviation.max(axis=1) > threshold]\n",
    "\n",
    "print(\"Regions with anomalous category distributions in A:\")\n",
    "print(flagged_regions)\n",
    "\n",
    "# 5. Optional: run chi-square test for independence (A vs B)\n",
    "contingency = pd.crosstab(df['B'], df['A'])\n",
    "chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "print(\"\\nChi-square test for independence:\")\n",
    "print(f\"Chi2 statistic = {chi2:.2f}, p-value = {p:.4g}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
